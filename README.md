# Tech Challenge 2 (Fase 2): Pipeline Batch Bovespa: Ingest√£o e Arquitetura de dados


**Tech Challenge** √© um projeto que re√∫ne a aplica√ß√£o dos conhecimentos adquiridos em todas as disciplinas de uma fase da Especializa√ß√£o em Machine Learning Engineering da FIAP PosTech.

Para o Tech Challenge 2, o desafio proposto foi o seguinte:

> üì¢ **Problema:** construa um pipeline de dados completo para **Extrair, Processar e Analisar dados do preg√£o da B3 (IBovespa)**, utilizando AWS S3, Glue, Lambda e Athena. Para acessar os dados, obrigatoriamente acessar o link: [Carteira Te√≥rica o IBovespa](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br)

Para este desafio as entregas devem ser realizadas utilizando tecnologias da **Amazon Cloud** e atender aos seguintes **Requisitos/objetivos**:

‚Ä¢ **Requisito 1:** realizar o scrap de dados do site da B3 (IBovespa), extraindo dados do preg√£o (dados brutos).

‚Ä¢ **Requisito 2:** os dados brutos devem ser ingeridos no S3 em formato parquet com parti√ß√£o di√°ria.

‚Ä¢ **Requisito 3:** o Bucket S3 deve acionar uma Lambda, que por sua vez ir√° chamar um job de ETL no Glue.

‚Ä¢ **Requisito 4:** a Lambda pode ser em qualquer linguagem. Ela apenas dever√° iniciar o job Glue.

‚Ä¢ **Requisito 5:** o job Glue deve ser feito no modo visual. Este job deve conter as seguintes transforma√ß√µes obrigat√≥rias:

5.a: Realizar agrupamento num√©rico, sumariza√ß√£o, contagem ou soma;

5.b: Renomear duas colunas existentes, al√©m das colunas de agrupamento;

5.c: Realizar um c√°lculo com campos de data; por exemplo, poder ser dura√ß√£o, compara√ß√£o ou diferen√ßa entre datas.

‚Ä¢ **Requisito 6:** os dados refinados no job Glue devem ser salvos no formato parquet em uma pasta chamada REFINED, particionados por data e pelo nome ou abrevia√ß√£o da a√ß√£o do preg√£o.

‚Ä¢ **Requisito 7:** o job Glue deve automaticamente catalogar o dado no Glue Catalog e criar uma tabela no banco de dados default do Glue Catalog.

‚Ä¢ **Requisito 8:** os dados devem estar dispon√≠veis e leg√≠veis no Athena.

‚Ä¢ **Requisito 9:** (OPCIONAL) construir um notebook no Athena para realizar uma visualiza√ß√£o gr√°fica dos dados ingeridos.

‚Ä¢ **Requisito 10:** (OPCIONAL) construir uma Pipeline Stream Bitcoin, conforme arquitetura de refer√™ncia fornecida.

## üìå Objetivos

- Elaborar a **Arquitetura do projeto**, demonstrando todas as fases da pipeline;
- Implementar as tecnologias para o atendimento dos requisitos da Pipeline de Ingest√£o de Dados da B3;
- Documentar o projeto de forma a permitir a sua reprodu√ß√£o;
- Disponibilizar a documenta√ß√£o em um reposit√≥rio no **GitHub**.

## Poss√≠veis dores

- Falta de automa√ß√£o na obten√ß√£o dos dados bem como seu tratamento;
- Falta de padroniza√ß√£o para acesso aos dados bem como tipos de retornos e formatos mais adequados para consumo na produ√ß√£o de consultas e analytics;
- Suporte e documenta√ß√£o insuficientes;
- Redund√¢ncia de dados em hist√≥rico confi√°vel, com fonte de dados pr√≥pria;
- Baixa capacidade de an√°lise em quest√µes relevantes para o usu√°rio final.

## Proposta de solu√ß√£o

Em face ao desafio proposto, algumas funcionalidades propostas (stages) para a Pipeline Batch Bovespa:

- Ingest√£o de dados: coleta autom√°tica de dados (webscraping) extra√≠dos do site B3 (IBovespa), por meio de script em Lambda trigado pelo Event Bridge;
- Armazenamento de dados: salvamento em formato bruto (RAW), com parti√ß√£o di√°ria, em Bucket S3;
- Processamento: limpeza, transforma√ß√£o e padroniza√ß√£o, usando script em Glue acionado pela Lambda e trigado por Event Bridge;
- Carga final: grava√ß√£o de dado procesado (REFINED), com parti√ß√£o di√°ria, em Bucket S3;
- Agrega√ß√µes e C√°lculos: agrega√ß√µes e c√°lculos, usando script em Glue, para an√°lise do comportamento dos dados;
- Consumo: leitura dos dados refinados e agregados para a produ√ß√£o de relat√≥rios e dashboards, usando Athena e Google Colab.



**Importante**

Toda a implementa√ß√£o foi feita via Terraform, portanto, com o princ√≠pio de **Infrastructure as a Code** e foi documentada neste reposit√≥rio.



### üìÇ Estrutura do projeto

(inserir)

### üî© Arquitetura da solu√ß√£o

A arquitetura da solu√ß√£o foi desenhada com base nos stages necess√°rios ao atendimento de requisitos e consta na pasta de documenta√ß√£o deste reposit√≥rio. [Link para o Diagrama](inserir figura e link)

## Documenta√ß√£o Terraform

```

# terraform-infra-pipeline-aws


- criar um repoist√≥rio no github
- abrir o reposit√≥rio em uma IDE
- criar diret√≥rio infra
- criar arquivo infra/main.tf

- criar arquivo infra/variables.tf

```bash
variable "bucket_name" {
  description = "The name of the S3 bucket to create."
  type        = string
  
}
```

- criar arquivo infra/provider.tf com o conte√∫do:

```bash
provider "aws" {
  region = "sa-east-1"
}
```
- criar o arquivo infra/backend.tf
- indica onde o arquivo statefile ser√° salvo
- par√¢metros ser√£o atualizados em tempo de pipeline
- criar os diret√≥rios infra/envs/dev, infra/envs/hom infra/envs/prod
- criar o arquivo terraform.tfvars dentro de cada um dos diret√≥rios com o conte√∫do

```bash
bucket_name = "<env>-sa-east-1-buildrun-video-pipeline"``

### Configurar Github actions e AWS

- https://aws.amazon.com/pt/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/

- Acessar a conta da AWS
- A conex√£o do github e aws ser√° entre o OpenIdentity
- Acessar o IAM > Identity provider > Add provider
- Preencha:

  - Provider URL: `https://token.actions.githubusercontent.com`
  - Audience: `sts.amazonaws.com`
- Clique no provider rec√©m criado:
![alt text](image.png)

- Quando a esteira executar, ela vai chamar a aws e ela vai passar algumas informa√ß√µes
- A AWS vai identificar que √© uma pipeline segura e vai dar as permiss√µes de acesso √† pipeline

- Clique em `Assign role`
![alt text](image-1.png)

- Clique em `Create a new role`
![alt text](image-2.png)

- Selecione `Web identity`
![alt text](image-3.png)

- Preencha GitHub organization com o nome da sua conta do GitHub
![alt text](image-4.png)

- Selecionar as permiss√µes que a esteira ter√° considerando a Boa pr√°tica de privil√©gio m√≠nimo,
se precisar de mais permiss√µes √© s√≥ incluir
```
AmazonS3FullAccess
AmazonDynamoDBFullAccess
```
![alt text](image-5.png)

- Inserir o nome da role e clicar em `Create role`
![alt text](image-6.png)


- Criar o bucket para armazenar o statefile
- Acessar o servi√ßo S3 > Create bucket > Atribuir o nome
- Habilitar o versionamento do bucker e clicar em `Create bucket`
![alt text](image-7.png)

- Criar uma tabele no DynamoDB para realizar o lock para modifica√ß√µes concorrentes
- Acessar o servi√ßo DynamoDB > Create table
![alt text](image-8.png)


- Table name: <nome da conta do github>-sa-east-1-terraform-lock
- Partition key: `LockID` # precisa seguir essa sintaxe
![alt text](image-9.png)
- Clicar em `Create table`

Feito isso, criar o arquivo .github/workflows/terraform.yaml, .github/workflows/develop.yaml, .github/workflows/prod.yaml

## V√≠deo de Apresenta√ß√£o no Youtube
Para melhor compreens√£o da entrega, foi produzido um v√≠deo de apresenta√ß√£o que foi publicado no Youtube:

[Link para a V√≠deo](inserir link)


## ‚úíÔ∏è Autores

| Nome                            |   RM    | Link do GitHub                                      |
|---------------------------------|---------|-----------------------------------------------------|
| Ana Paula de Almeida            | 363602  | [GitHub](https://github.com/Ana9873P)               |
| Augusto do Nascimento Omena     | 363185  | [GitHub](https://github.com/AugustoOmena)           |
| Bruno Gabriel de Oliveira       | 361248  | [GitHub](https://github.com/brunogabrieldeoliveira) |
| Jos√© Walmir Gon√ßalves Duque     | 363196  | [GitHub](https://github.com/WALMIRDUQUE)            |
| Pedro Henrique da Costa Ulisses | 360864  | [GitHub](https://github.com/ordepzero)              |

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT.  
Consulte o arquivo [license](docs/license/license.txt)  para mais detalhes.